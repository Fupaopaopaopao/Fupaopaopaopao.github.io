

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Fuchsia">
  <meta name="keywords" content="">
  
    <meta name="description" content="HMM: hidden markov model词性只跟上一个词性有关，词只和本位置的词性有关。Hidden是因为词性是不知道的。能观察到的是词，通过词反推获得词性建立联系。 也就是说我们想知道两个概率：     这个位置是这个词性的概率 Transition prob (A): P(wi|ti) 这个词性是这个词的概率 Emission prob (O): P(ti-1|ti)  因此，Outp">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP week5: HMM">
<meta property="og:url" content="http://example.com/2024/04/26/NLP-week5-HMM-and-NN/index.html">
<meta property="og:site_name" content="Fuchsia&#39;s BoLg">
<meta property="og:description" content="HMM: hidden markov model词性只跟上一个词性有关，词只和本位置的词性有关。Hidden是因为词性是不知道的。能观察到的是词，通过词反推获得词性建立联系。 也就是说我们想知道两个概率：     这个位置是这个词性的概率 Transition prob (A): P(wi|ti) 这个词性是这个词的概率 Emission prob (O): P(ti-1|ti)  因此，Outp">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/hmm.png">
<meta property="og:image" content="http://example.com/img/maxesti-5.png">
<meta property="og:image" content="http://example.com/img/tahn.png">
<meta property="og:image" content="http://example.com/img/nn.png">
<meta property="og:image" content="http://example.com/img/n.png">
<meta property="og:image" content="http://example.com/img/nn-struc.png">
<meta property="article:published_time" content="2024-04-26T08:39:32.000Z">
<meta property="article:modified_time" content="2024-05-06T19:31:04.252Z">
<meta property="article:author" content="Fuchsia">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content="natural language processing">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/hmm.png">
  
  
  
  <title>NLP week5: HMM - Fuchsia&#39;s BoLg</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fuchsia&lt;333</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/back.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="NLP week5: HMM"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-04-26 18:39" pubdate>
          April 26, 2024 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          1.7k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          15 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">NLP week5: HMM</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="HMM-hidden-markov-model"><a href="#HMM-hidden-markov-model" class="headerlink" title="HMM: hidden markov model"></a>HMM: hidden markov model</h1><p><img src="/img/hmm.png" srcset="/img/loading.gif" lazyload alt="hmm"><br>词性只跟上一个词性有关，词只和本位置的词性有关。Hidden是因为词性是不知道的。能观察到的是词，通过词反推获得词性建立联系。</p>
<p>也就是说我们想知道两个概率：   </p>
<ol>
<li>这个位置是这个词性的概率 Transition prob (A): P(wi|ti)</li>
<li>这个词性是这个词的概率 Emission prob (O): P(ti-1|ti)</li>
</ol>
<p>因此，<br>Output independence:<br><strong>假设： 每个word只依赖于tag, word之间是绝对独立的</strong><br>$$<br>P(\vec{w} \mid \vec{t})&#x3D;\prod_{i&#x3D;1}^n P\left(w_i \mid t_i\right)<br>$$</p>
<p>Markov assimption:<br>在HMM中，当前的tag只跟前一个有关 （类似于N-gram）<br>$$<br>P(\vec{t})&#x3D;\prod_{i&#x3D;1}^n P\left(t_i \mid t_{i-1}\right)<br>$$</p>
<p>Maximum likelihood estimation:<br><img src="/img/maxesti-5.png" srcset="/img/loading.gif" lazyload alt="hmm"><br>$$<br>\begin{aligned}<br>\hat{t} &amp; &#x3D;\operatorname{argmax}<em>t P(\vec{w} \mid \vec{t}) P(\vec{t}) \<br>&amp; &#x3D;\operatorname{argmax}<em>t \prod</em>{i&#x3D;1}^n P\left(w_i \mid t_i\right) P\left(t_1 \mid t</em>{i-1}\right)<br>\end{aligned}<br>$$<br>Core idea : 取所有tag sequences组合中概率最高的</p>
<h2 id="Viterbi-算法"><a href="#Viterbi-算法" class="headerlink" title="Viterbi 算法"></a>Viterbi 算法</h2><p>计算过程：一般题目里面会给两个表格，分别为Table A and Table O, 这两个表记录着计算需要的两个概率。这时我们需要计算第三个表并得出<strong>所有sequence中概率最高的</strong>。</p>
<h3 id="计算tag-sequence的概率"><a href="#计算tag-sequence的概率" class="headerlink" title="计算tag sequence的概率"></a>计算tag sequence的概率</h3><table>
<thead>
<tr>
<th></th>
<th>Janet</th>
<th>will</th>
<th>back</th>
<th>the</th>
<th>bill</th>
</tr>
</thead>
<tbody><tr>
<td>NNP</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MD</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>VB</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>JJ</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>NN</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>RB</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>RB</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>表格中的第一列，都为最基础的，例如第一个单元格中， P(Janet|nnp) x P(NNP|s) &#x3D; Score<br>而第一行的第二个单元格中则需要计算前一个位置的每个词性的score，并取最大值<br>假设，我们现在计算前一个词性为NNP，<br><strong>will,NNP score &#x3D; max((will|nnp) x P(NNP|tJanet) x score(tJanet,Janet))</strong><br><strong>根据最后一列最大的score往前回溯</strong>  </p>
<p>时间复杂度 $$O\left(T^2 N\right)$$<br>T: size of the tagset, rows<br>N: length of the sequence, columns<br>遍历整个表格是T*N，然后后面为了取最大值每个格子中都是N，所以是以上式子</p>
<h1 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h1><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><p>主要是为了把linear function来Capture X,Y的关系，比如将一个linear function输出为一个RANGE(-1,1)的向量，同时有很多不同种类的激活函数。<br>Sigmoid中包括logistic function (range(0,1)), tanh(range(-1,1))<br>RuLe:RuLe的计算更简洁</p>
<h3 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h3><p>$$<br>\underset{}{h&#x3D;\tanh }\left(\sum_j w_j x_j+b\right)<br>$$<br><img src="/img/tahn.png" srcset="/img/loading.gif" lazyload alt="tahn">   </p>
<h2 id="Neural-Network的基本结构"><a href="#Neural-Network的基本结构" class="headerlink" title="Neural Network的基本结构"></a>Neural Network的基本结构</h2><p>很早之前就有NN，刚开始算力消耗太大并，overfitting严重 ，并行计算促进NN的发展。</p>
<blockquote>
<p>Note: 超过两层就可以算deep learning  </p>
</blockquote>
<p><img src="/img/nn.png" srcset="/img/loading.gif" lazyload alt="nn"><br>中间的hidden layer越多捕捉到的关系越复杂，<br>每一层中的node，分别capture一部分的non-linear relationship。<strong>Fully connected</strong> 就是上一层和下一层的每一个节点都连接。<br><img src="/img/n.png" srcset="/img/loading.gif" lazyload alt="n">   </p>
<h3 id="Output-layer"><a href="#Output-layer" class="headerlink" title="Output layer"></a>Output layer</h3><p>输出一共有两种，分别是binary classification和multinomial classification(multi-class classification)</p>
<ul>
<li><strong>Binary classification</strong><br>输出层只有一个节点，使用tahn将上一层的函数输入为一个RANGE(-1,1)的值，if -1&lt;y&lt;0, neg; if1&gt;y&gt;&#x3D;0, pos.<blockquote>
<p>Note: 我们知道如果想把一个(1,5)的矩阵转化成(1,1)的矩阵，那么w也就是系数矩阵必须形状为(5,1)</p>
</blockquote>
</li>
<li><strong>Multinomial classification</strong><br>Y &#x3D; [Y1, Y2, Y3]<br>这个向量指的是：distribution over all possible classes.<br>我们使用SOFTMAX作为最后一层到输出层的激活函数，将一个和不为1的向量转换成概率分布。然后我们取概率最大的分类为结果。</li>
</ul>
<h3 id="Loss-function-Gradient-descent"><a href="#Loss-function-Gradient-descent" class="headerlink" title="Loss function &amp; Gradient descent"></a>Loss function &amp; Gradient descent</h3><p>详见appendix</p>
<h3 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h3><p>parameters多了会导致过拟合，这是因为训练集太细且包括noise，因此我们必须通过Regularization来消除overfitting的影响，并且增加一个penalty component来避免某个系数过大,它的作用是推动模型的参数尽量趋向于零，从而促使模型产生稀疏解，有助于特征选择和模型简化。</p>
<p>L1-norm: sum of absolute of all parameters<br>L2-norm: sum of squares of all parameters</p>
<p><strong>特点：</strong></p>
<ul>
<li>L1-norm倾向于产生稀疏解，因为它的优化过程中，一些参数会变为零，因此在稀疏模型和特征选择中应用较多。</li>
<li>L2-norm倾向于产生较小的稠密解，因为它对参数的惩罚更加均衡，避免了一些参数过大的情况，常用于优化算法中的正则化。</li>
</ul>
<p><strong>鲁棒性：</strong></p>
<ul>
<li>L1-norm对于异常值（outliers）有较好的鲁棒性，因为它不受离群值的影响。</li>
<li>L2-norm对于异常值的鲁棒性较差，因为它会受到异常值的平方的影响</li>
</ul>
<p>$$<br>\operatorname{Loss}&#x3D;-\log (L)+\lambda \Sigma\left|w_r\right|<br>$$<br>lambda是系数,后面一项是penalty compont（前一项就是普通的LOSS）</p>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>随机扔掉一些节点 增加noise<br>dropout rate &#x3D; 0.1 随机干掉0.2的神经元</p>
<h2 id="NN的topic-classification的过程"><a href="#NN的topic-classification的过程" class="headerlink" title="NN的topic classification的过程"></a>NN的topic classification的过程</h2><p>任务：输入一个文档，输出一个topic<br><img src="/img/nn-struc.png" srcset="/img/loading.gif" lazyload alt="nn-s"><br>Input：bag-of-words这个就是每个词的频率<br>Output: y-pred &#x3D; [0.2, 0.1, 0.7] eg. 比如说分别是 eco, pol, spo  </p>
<h3 id="训练模型："><a href="#训练模型：" class="headerlink" title="训练模型："></a>训练模型：</h3><ul>
<li>将数据输入模型，通过前向传播计算模型输出，并计算损失值。</li>
<li>通过反向传播计算梯度，并根据选择的优化器更新模型参数。</li>
<li>反复迭代这个过程，直到损失值收敛或达到设定的停止条件</li>
</ul>
<h2 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h2><p>任务：通过n-gram，比如输入前两个单词，输出后一个单词。这时我们需要用单词的embeddings代替bow，那么我们如何获得embeddings。<br>有趣的是，我们的topic classification的w1就是可以代表单词的向量。当我们需要输入前两个单词时就是把代表这两个单词的向量concatenate一下，比如word1 &#x3D; [0,1], word2 &#x3D; [0,2], 那么输入的向量就是[0,1,0,2]，<strong>值得注意的是，这种方法保留了文字顺序的信息。</strong></p>
<p>输出：所有单词在这里出现的概率</p>
<h2 id="对比NN-based-and-count-based-n-gram"><a href="#对比NN-based-and-count-based-n-gram" class="headerlink" title="对比NN-based and count-based n-gram"></a>对比NN-based and count-based n-gram</h2><p>后者不需要大量算力，可以手算，且在大型的datasets上会over sparse（过于稀疏，因为很多count是0）<br>NN则可以capture到单词的grammar or semantic等信息 （film vs movie）</p>
<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>当处理多分类问题时，常用的损失函数是交叉熵损失函数（Cross-Entropy Loss）。在给定样本和模型输出概率分布向量 (\hat{y} &#x3D; [\hat{y}_1, \hat{y}_2, …, \hat{y}_n]) 的情况下，交叉熵损失函数可以用来衡量模型的预测与真实标签之间的差异。</p>
<p>具体地，在给定正确的标签向量 (y &#x3D; [0, 1, 0, …, 0])，表示样本属于第 (j) 个类别的情况下，如果模型的预测概率 (\hat{y}_j) 是最大的，即 (j) 是正确的类别，那么交叉熵损失函数的值会接近于 0；反之，如果 (j) 不是正确的类别，则损失函数的值会较大。</p>
<p>对于你提供的例子，假设正确的类别是第二个类别，即输出标签向量为 (y &#x3D; [0, 1, 0, …, 0])，而模型的预测概率向量为 (\hat{y} &#x3D; [0.2, 0.7, 0.3, …, 0])，那么对应的交叉熵损失函数可以表示为：</p>
<p>[\text{Loss} &#x3D; - (0 \times \log(0.2) + 1 \times \log(0.7) + 0 \times \log(0.3) + …)]</p>
<p>[&#x3D; - \log(0.7)]</p>
<p>因此，在这个样本中，损失函数的值为负对数预测概率 (\log(0.7)) 的负值。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/nlp/" class="category-chain-item">nlp</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/nlp/" class="print-no-link">#nlp</a>
      
        <a href="/tags/natural-language-processing/" class="print-no-link">#natural language processing</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>NLP week5: HMM</div>
      <div>http://example.com/2024/04/26/NLP-week5-HMM-and-NN/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Fuchsia</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>April 26, 2024</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/05/06/CNN/" title="CNN">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CNN</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/01/23/reactive-in-Vue3-0/" title="Reactive in Vue3.0">
                        <span class="hidden-mobile">Reactive in Vue3.0</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
